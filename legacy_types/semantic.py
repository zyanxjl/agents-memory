"""è¯­ä¹‰è®°å¿†å®ç°

ç»“åˆå‘é‡æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±çš„æ··åˆè¯­ä¹‰è®°å¿†ï¼Œä½¿ç”¨ï¼š
- HuggingFace ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥
- å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢è¿›è¡Œå¿«é€Ÿåˆç­›
- çŸ¥è¯†å›¾è°±è¿›è¡Œå®ä½“å…³ç³»æ¨ç†
- æ··åˆæ£€ç´¢ç­–ç•¥ä¼˜åŒ–ç»“æœè´¨é‡
"""

from typing import List, Dict, Any, Optional, Set, Tuple
from datetime import datetime, timedelta
import json
import logging
import math
import numpy as np

from ..base import BaseMemory, MemoryItem, MemoryConfig
from ..embedding import get_text_embedder, get_dimension
from ...core.database_config import get_database_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Entity:
    """å®ä½“ç±»"""
    
    def __init__(
        self,
        entity_id: str,
        name: str,
        entity_type: str = "MISC",
        description: str = "",
        properties: Dict[str, Any] = None
    ):
        self.entity_id = entity_id
        self.name = name
        self.entity_type = entity_type  # PERSON, ORG, PRODUCT, SKILL, CONCEPTç­‰
        self.description = description
        self.properties = properties or {}
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        self.frequency = 1  # å‡ºç°é¢‘ç‡
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "entity_id": self.entity_id,
            "name": self.name,
            "entity_type": self.entity_type,
            "description": self.description,
            "properties": self.properties,
            "frequency": self.frequency
        }

class Relation:
    """å…³ç³»ç±»"""
    
    def __init__(
        self,
        from_entity: str,
        to_entity: str,
        relation_type: str,
        strength: float = 1.0,
        evidence: str = "",
        properties: Dict[str, Any] = None
    ):
        self.from_entity = from_entity
        self.to_entity = to_entity
        self.relation_type = relation_type
        self.strength = strength
        self.evidence = evidence  # æ”¯æŒè¯¥å…³ç³»çš„åŸæ–‡æœ¬
        self.properties = properties or {}
        self.created_at = datetime.now()
        self.frequency = 1  # å…³ç³»å‡ºç°é¢‘ç‡
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "from_entity": self.from_entity,
            "to_entity": self.to_entity,
            "relation_type": self.relation_type,
            "strength": self.strength,
            "evidence": self.evidence,
            "properties": self.properties,
            "frequency": self.frequency
        }


class SemanticMemory(BaseMemory):
    """å¢å¼ºè¯­ä¹‰è®°å¿†å®ç°
    
    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨HuggingFaceä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥
    - å‘é‡æ£€ç´¢è¿›è¡Œå¿«é€Ÿç›¸ä¼¼åº¦åŒ¹é…
    - çŸ¥è¯†å›¾è°±å­˜å‚¨å®ä½“å’Œå…³ç³»
    - æ··åˆæ£€ç´¢ç­–ç•¥ï¼šå‘é‡+å›¾+è¯­ä¹‰æ¨ç†
    """
    
    def __init__(self, config: MemoryConfig, storage_backend=None):
        super().__init__(config, storage_backend)
        
        # åµŒå…¥æ¨¡å‹ï¼ˆç»Ÿä¸€æä¾›ï¼‰
        self.embedding_model = None
        self._init_embedding_model()
        
        # ä¸“ä¸šæ•°æ®åº“å­˜å‚¨
        self.vector_store = None
        self.graph_store = None
        self._init_databases()
        
        # å®ä½“å’Œå…³ç³»ç¼“å­˜ (ç”¨äºå¿«é€Ÿè®¿é—®)
        self.entities: Dict[str, Entity] = {}
        self.relations: List[Relation] = []
        
        # å®ä½“è¯†åˆ«å™¨
        self.nlp = None
        self._init_nlp()
        
        # è®°å¿†å­˜å‚¨
        self.semantic_memories: List[MemoryItem] = []
        self.memory_embeddings: Dict[str, np.ndarray] = {}
        
        logger.info("å¢å¼ºè¯­ä¹‰è®°å¿†åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨Qdrant+Neo4jä¸“ä¸šæ•°æ®åº“ï¼‰")
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–ç»Ÿä¸€åµŒå…¥æ¨¡å‹ï¼ˆç”± embedding_provider ç®¡ç†ï¼‰ã€‚"""
        try:
            self.embedding_model = get_text_embedder()
            # è½»é‡å¥åº·æ£€æŸ¥ä¸æ—¥å¿—
            try:
                test_vec = self.embedding_model.encode("health_check")
                dim = getattr(self.embedding_model, "dimension", len(test_vec))
                logger.info(f"âœ… åµŒå…¥æ¨¡å‹å°±ç»ªï¼Œç»´åº¦: {dim}")
            except Exception:
                logger.info("âœ… åµŒå…¥æ¨¡å‹å°±ç»ª")
        except Exception as e:
            logger.error(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_databases(self):
        """åˆå§‹åŒ–ä¸“ä¸šæ•°æ®åº“å­˜å‚¨"""
        try:
            # è·å–æ•°æ®åº“é…ç½®
            db_config = get_database_config()
            
            # åˆå§‹åŒ–Qdrantå‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨è¿æ¥ç®¡ç†å™¨é¿å…é‡å¤è¿æ¥ï¼‰
            from ..storage.qdrant_store import QdrantConnectionManager
            qdrant_config = db_config.get_qdrant_config() or {}
            qdrant_config["vector_size"] = get_dimension()
            self.vector_store = QdrantConnectionManager.get_instance(**qdrant_config)
            logger.info("âœ… Qdrantå‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–Neo4jå›¾æ•°æ®åº“
            from ..storage.neo4j_store import Neo4jGraphStore
            neo4j_config = db_config.get_neo4j_config()
            self.graph_store = Neo4jGraphStore(**neo4j_config)
            logger.info("âœ… Neo4jå›¾æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
            # éªŒè¯è¿æ¥
            vector_health = self.vector_store.health_check()
            graph_health = self.graph_store.health_check()
            
            if not vector_health:
                logger.warning("âš ï¸ Qdrantè¿æ¥å¼‚å¸¸ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
            if not graph_health:
                logger.warning("âš ï¸ Neo4jè¿æ¥å¼‚å¸¸ï¼Œå›¾æœç´¢åŠŸèƒ½å¯èƒ½å—é™")
            
            logger.info(f"ğŸ¥ æ•°æ®åº“å¥åº·çŠ¶æ€: Qdrant={'âœ…' if vector_health else 'âŒ'}, Neo4j={'âœ…' if graph_health else 'âŒ'}")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®åº“é…ç½®å’Œç½‘ç»œè¿æ¥")
            logger.info("ğŸ’¡ å‚è€ƒ DATABASE_SETUP_GUIDE.md è¿›è¡Œé…ç½®")
            raise
    
    def _init_nlp(self):
        """åˆå§‹åŒ–NLPå¤„ç†å™¨ - æ™ºèƒ½å¤šè¯­è¨€æ”¯æŒ"""
        try:
            import spacy
            self.nlp_models = {}
            
            # å°è¯•åŠ è½½å¤šè¯­è¨€æ¨¡å‹
            models_to_try = [
                ("zh_core_web_sm", "ä¸­æ–‡"),
                ("en_core_web_sm", "è‹±æ–‡")
            ]
            
            loaded_models = []
            for model_name, lang_name in models_to_try:
                try:
                    nlp = spacy.load(model_name)
                    self.nlp_models[model_name] = nlp
                    loaded_models.append(lang_name)
                    logger.info(f"âœ… åŠ è½½{lang_name}spaCyæ¨¡å‹: {model_name}")
                except OSError:
                    logger.warning(f"âš ï¸ {lang_name}spaCyæ¨¡å‹ä¸å¯ç”¨: {model_name}")
            
            # è®¾ç½®ä¸»è¦NLPå¤„ç†å™¨
            if "zh_core_web_sm" in self.nlp_models:
                self.nlp = self.nlp_models["zh_core_web_sm"]
                logger.info("ğŸ¯ ä¸»è¦ä½¿ç”¨ä¸­æ–‡spaCyæ¨¡å‹")
            elif "en_core_web_sm" in self.nlp_models:
                self.nlp = self.nlp_models["en_core_web_sm"]
                logger.info("ğŸ¯ ä¸»è¦ä½¿ç”¨è‹±æ–‡spaCyæ¨¡å‹")
            else:
                self.nlp = None
                logger.warning("âš ï¸ æ— å¯ç”¨spaCyæ¨¡å‹ï¼Œå®ä½“æå–å°†å—é™")
            
            if loaded_models:
                logger.info(f"ğŸ“š å¯ç”¨è¯­è¨€æ¨¡å‹: {', '.join(loaded_models)}")
                
        except ImportError:
            logger.warning("âš ï¸ spaCyä¸å¯ç”¨ï¼Œå®ä½“æå–å°†å—é™")
            self.nlp = None
            self.nlp_models = {}
    
    def add(self, memory_item: MemoryItem) -> str:
        """æ·»åŠ è¯­ä¹‰è®°å¿†"""
        try:
            # 1. ç”Ÿæˆæ–‡æœ¬åµŒå…¥
            embedding = self.embedding_model.encode(memory_item.content)
            self.memory_embeddings[memory_item.id] = embedding
            
            # 2. æå–å®ä½“å’Œå…³ç³»
            entities = self._extract_entities(memory_item.content)
            relations = self._extract_relations(memory_item.content, entities)
            
            # 3. å­˜å‚¨åˆ°Neo4jå›¾æ•°æ®åº“
            for entity in entities:
                self._add_entity_to_graph(entity, memory_item)
            
            for relation in relations:
                self._add_relation_to_graph(relation, memory_item)
            
            # 4. å­˜å‚¨åˆ°Qdrantå‘é‡æ•°æ®åº“
            metadata = {
                "memory_id": memory_item.id,
                "user_id": memory_item.user_id,
                "content": memory_item.content,
                "memory_type": memory_item.memory_type,
                "timestamp": int(memory_item.timestamp.timestamp()),
                "importance": memory_item.importance,
                "entities": [e.entity_id for e in entities],
                "entity_count": len(entities),
                "relation_count": len(relations)
            }
            
            success = self.vector_store.add_vectors(
                vectors=[embedding.tolist()],
                metadata=[metadata],
                ids=[memory_item.id]
            )
            
            if not success:
                logger.warning("âš ï¸ å‘é‡å­˜å‚¨å¤±è´¥ï¼Œä½†è®°å¿†å·²æ·»åŠ åˆ°å›¾æ•°æ®åº“")
            
            # 5. æ·»åŠ å®ä½“ä¿¡æ¯åˆ°å…ƒæ•°æ®
            memory_item.metadata["entities"] = [e.entity_id for e in entities]
            memory_item.metadata["relations"] = [
                f"{r.from_entity}-{r.relation_type}-{r.to_entity}" for r in relations
            ]
            
            # 6. å­˜å‚¨è®°å¿†
            self.semantic_memories.append(memory_item)
            
            logger.info(f"âœ… æ·»åŠ è¯­ä¹‰è®°å¿†: {len(entities)}ä¸ªå®ä½“, {len(relations)}ä¸ªå…³ç³»")
            return memory_item.id
        
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è¯­ä¹‰è®°å¿†å¤±è´¥: {e}")
            raise
    
    def retrieve(self, query: str, limit: int = 5, **kwargs) -> List[MemoryItem]:
        """æ£€ç´¢è¯­ä¹‰è®°å¿†"""
        try:
            user_id = kwargs.get("user_id")

            # 1. å‘é‡æ£€ç´¢
            vector_results = self._vector_search(query, limit * 2, user_id)
            
            # 2. å›¾æ£€ç´¢
            graph_results = self._graph_search(query, limit * 2, user_id)
            
            # 3. æ··åˆæ’åº
            combined_results = self._combine_and_rank_results(
                vector_results, graph_results, query, limit
            )

            # 3.1 è®¡ç®—æ¦‚ç‡ï¼ˆå¯¹ combined_score åš softmax å½’ä¸€åŒ–ï¼‰
            scores = [r.get("combined_score", r.get("vector_score", 0.0)) for r in combined_results]
            if scores:
                import math
                max_s = max(scores)
                exps = [math.exp(s - max_s) for s in scores]
                denom = sum(exps) or 1.0
                probs = [e / denom for e in exps]
            else:
                probs = []
            
            # 4. è¿‡æ»¤å·²é—å¿˜è®°å¿†å¹¶è½¬æ¢ä¸ºMemoryItem
            result_memories = []
            for idx, result in enumerate(combined_results):
                memory_id = result.get("memory_id")
                
                # æ£€æŸ¥æ˜¯å¦å·²é—å¿˜
                memory = next((m for m in self.semantic_memories if m.id == memory_id), None)
                if memory and memory.metadata.get("forgotten", False):
                    continue  # è·³è¿‡å·²é—å¿˜çš„è®°å¿†
                
                # å¤„ç†æ—¶é—´æˆ³
                timestamp = result.get("timestamp")
                if isinstance(timestamp, str):
                    try:
                        timestamp = datetime.fromisoformat(timestamp)
                    except ValueError:
                        timestamp = datetime.now()
                elif isinstance(timestamp, (int, float)):
                    timestamp = datetime.fromtimestamp(timestamp)
                else:
                    timestamp = datetime.now()
                
                # ç›´æ¥ä»ç»“æœæ•°æ®æ„å»ºMemoryItemï¼ˆé™„å¸¦åˆ†æ•°ä¸æ¦‚ç‡ï¼‰
                memory_item = MemoryItem(
                    id=result["memory_id"],
                    content=result["content"],
                    memory_type="semantic",
                    user_id=result.get("user_id", "default"),
                    timestamp=timestamp,
                    importance=result.get("importance", 0.5),
                    metadata={
                        **result.get("metadata", {}),
                        "combined_score": result.get("combined_score", 0.0),
                        "vector_score": result.get("vector_score", 0.0),
                        "graph_score": result.get("graph_score", 0.0),
                        "probability": probs[idx] if idx < len(probs) else 0.0,
                    }
                )
                result_memories.append(memory_item)
            
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(result_memories)} æ¡ç›¸å…³è®°å¿†")
            return result_memories[:limit]
                
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢è¯­ä¹‰è®°å¿†å¤±è´¥: {e}")
            return []
    
    def _vector_search(self, query: str, limit: int, user_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """Qdrantå‘é‡æœç´¢"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode(query)
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            where_filter = {"memory_type": "semantic"}
            if user_id:
                where_filter["user_id"] = user_id

            # Qdrantå‘é‡æ£€ç´¢
            results = self.vector_store.search_similar(
                query_vector=query_embedding.tolist(),
                limit=limit,
                where=where_filter if where_filter else None
            )

            # è½¬æ¢ç»“æœæ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            formatted_results = []
            for result in results:
                formatted_result = {
                    "id": result["id"],
                    "score": result["score"],
                    **result["metadata"]  # åŒ…å«æ‰€æœ‰å…ƒæ•°æ®
                }
                formatted_results.append(formatted_result)

            logger.debug(f"ğŸ” Qdrantå‘é‡æœç´¢è¿”å› {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results
                
        except Exception as e:
            logger.error(f"âŒ Qdrantå‘é‡æœç´¢å¤±è´¥: {e}")
            return []

    def _graph_search(self, query: str, limit: int, user_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """Neo4jå›¾æœç´¢"""
        try:
            # ä»æŸ¥è¯¢ä¸­æå–å®ä½“
            query_entities = self._extract_entities(query)
            
            if not query_entities:
                # å¦‚æœæ²¡æœ‰æå–åˆ°å®ä½“ï¼Œå°è¯•æŒ‰åç§°æœç´¢
                entities_by_name = self.graph_store.search_entities_by_name(
                    name_pattern=query, 
                    limit=10
                )
                if entities_by_name:
                    query_entities = [Entity(
                        entity_id=e["id"],
                        name=e["name"],
                        entity_type=e["type"]
                    ) for e in entities_by_name[:3]]
                else:
                    return []
            
            # åœ¨Neo4jå›¾ä¸­æŸ¥æ‰¾ç›¸å…³å®ä½“å’Œè®°å¿†
            related_memory_ids = set()
            
            for entity in query_entities:
                try:
                    # æŸ¥æ‰¾ç›¸å…³å®ä½“
                    related_entities = self.graph_store.find_related_entities(
                        entity_id=entity.entity_id,
                        max_depth=2,
                        limit=20
                    )
                    
                    # æ”¶é›†ç›¸å…³è®°å¿†ID
                    for rel_entity in related_entities:
                        if "memory_id" in rel_entity:
                            related_memory_ids.add(rel_entity["memory_id"])
                    
                    # ä¹Ÿæ·»åŠ ç›´æ¥åŒ¹é…çš„å®ä½“è®°å¿†
                    entity_rels = self.graph_store.get_entity_relationships(entity.entity_id)
                    for rel in entity_rels:
                        rel_data = rel.get("relationship", {})
                        if "memory_id" in rel_data:
                            related_memory_ids.add(rel_data["memory_id"])
                            
                except Exception as e:
                    logger.debug(f"å›¾æœç´¢å®ä½“ {entity.entity_id} å¤±è´¥: {e}")
                    continue
            
            # æ„å»ºç»“æœ - ä»å‘é‡æ•°æ®åº“è·å–å®Œæ•´è®°å¿†ä¿¡æ¯
            results = []
            for memory_id in list(related_memory_ids)[:limit * 2]:  # è·å–æ›´å¤šå€™é€‰
                try:
                    # ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜è·å–è®°å¿†è¯¦æƒ…ï¼Œé¿å…å ä½å‘é‡ç»´åº¦ä¸ä¸€è‡´é—®é¢˜
                    mem = self._find_memory_by_id(memory_id)
                    if not mem:
                        continue

                    if user_id and mem.user_id != user_id:
                        continue

                    metadata = {
                        "content": mem.content,
                        "user_id": mem.user_id,
                        "memory_type": mem.memory_type,
                        "importance": mem.importance,
                        "timestamp": int(mem.timestamp.timestamp()),
                        "entities": mem.metadata.get("entities", [])
                    }

                    # è®¡ç®—å›¾ç›¸å…³æ€§åˆ†æ•°
                    graph_score = self._calculate_graph_relevance_neo4j(metadata, query_entities)

                    results.append({
                        "id": memory_id,
                        "memory_id": memory_id,
                        "content": metadata.get("content", ""),
                        "similarity": graph_score,
                        "user_id": metadata.get("user_id"),
                        "memory_type": metadata.get("memory_type"),
                        "importance": metadata.get("importance", 0.5),
                        "timestamp": metadata.get("timestamp"),
                        "entities": metadata.get("entities", [])
                    })

                except Exception as e:
                    logger.debug(f"è·å–è®°å¿† {memory_id} è¯¦æƒ…å¤±è´¥: {e}")
                    continue
            
            # æŒ‰å›¾ç›¸å…³æ€§æ’åº
            results.sort(key=lambda x: x["similarity"], reverse=True)
            logger.debug(f"ğŸ•¸ï¸ Neo4jå›¾æœç´¢è¿”å› {len(results)} ä¸ªç»“æœ")
            return results[:limit]
            
        except Exception as e:
            logger.error(f"âŒ Neo4jå›¾æœç´¢å¤±è´¥: {e}")
            return []

    def _combine_and_rank_results(
        self,
        vector_results: List[Dict[str, Any]],
        graph_results: List[Dict[str, Any]],
        query: str,
        limit: int
    ) -> List[Dict[str, Any]]:
        """æ··åˆæ’åºç»“æœ - ä»…åŸºäºå‘é‡ä¸å›¾åˆ†æ•°çš„ç®€å•èåˆ"""
        # åˆå¹¶ç»“æœï¼ŒæŒ‰å†…å®¹å»é‡
        combined = {}
        content_seen = set()  # ç”¨äºå†…å®¹å»é‡
        
        # æ·»åŠ å‘é‡ç»“æœ
        for result in vector_results:
            memory_id = result["memory_id"]
            content = result.get("content", "")
            
            # å†…å®¹å»é‡ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„å†…å®¹
            content_hash = hash(content.strip())
            if content_hash in content_seen:
                logger.debug(f"âš ï¸ è·³è¿‡é‡å¤å†…å®¹: {content[:30]}...")
                continue
            
            content_seen.add(content_hash)
            combined[memory_id] = {
                **result,
                "vector_score": result.get("score", 0.0), 
                "graph_score": 0.0,
                "content_hash": content_hash
            }
        
        # æ·»åŠ å›¾ç»“æœ
        for result in graph_results:
            memory_id = result["memory_id"]
            content = result.get("content", "")
            content_hash = hash(content.strip())
            
            if memory_id in combined:
                combined[memory_id]["graph_score"] = result.get("similarity", 0.0)
            elif content_hash not in content_seen:
                content_seen.add(content_hash)
                combined[memory_id] = {
                    **result,
                    "vector_score": 0.0,
                    "graph_score": result.get("similarity", 0.0),
                    "content_hash": content_hash
                }
        
        # è®¡ç®—æ··åˆåˆ†æ•°ï¼šç›¸ä¼¼åº¦ä¸ºä¸»ï¼Œé‡è¦æ€§ä¸ºè¾…åŠ©æ’åºå› å­
        for memory_id, result in combined.items():
            vector_score = result["vector_score"]
            graph_score = result["graph_score"]
            importance = result.get("importance", 0.5)
            
            # æ–°è¯„åˆ†ç®—æ³•ï¼šå‘é‡æ£€ç´¢çº¯åŸºäºç›¸ä¼¼åº¦ï¼Œé‡è¦æ€§ä½œä¸ºåŠ æƒå› å­
            # åŸºç¡€ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆä¸å—é‡è¦æ€§å½±å“ï¼‰
            base_relevance = vector_score * 0.7 + graph_score * 0.3
            
            # é‡è¦æ€§ä½œä¸ºä¹˜æ³•åŠ æƒå› å­ï¼ŒèŒƒå›´ [0.8, 1.2]
            # importance in [0,1] -> weight in [0.8,1.2]
            importance_weight = 0.8 + (importance * 0.4)
            
            # æœ€ç»ˆå¾—åˆ†ï¼šç›¸ä¼¼åº¦ * é‡è¦æ€§æƒé‡
            combined_score = base_relevance * importance_weight
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæŸ¥çœ‹åˆ†æ•°åˆ†è§£
            result["debug_info"] = {
                "base_relevance": base_relevance,
                "importance_weight": importance_weight,
                "combined_score": combined_score
            }

            result["combined_score"] = combined_score
        
        # åº”ç”¨æœ€å°ç›¸å…³æ€§é˜ˆå€¼
        min_threshold = 0.1  # æœ€å°ç›¸å…³æ€§é˜ˆå€¼
        filtered_results = [
            result for result in combined.values() 
            if result["combined_score"] >= min_threshold
        ]

        # æ’åºå¹¶è¿”å›
        sorted_results = sorted(
            filtered_results,
            key=lambda x: x["combined_score"],
            reverse=True
        )
        
        # è°ƒè¯•ä¿¡æ¯
        logger.debug(f"ğŸ” å‘é‡ç»“æœ: {len(vector_results)}, å›¾ç»“æœ: {len(graph_results)}")
        logger.debug(f"ğŸ“ å»é‡å: {len(combined)}, è¿‡æ»¤å: {len(filtered_results)}")
        
        if logger.level <= logging.DEBUG:
            for i, result in enumerate(sorted_results[:3]):
                logger.debug(f"  ç»“æœ{i+1}: å‘é‡={result['vector_score']:.3f}, å›¾={result['graph_score']:.3f}, ç²¾ç¡®={result.get('exact_match_bonus', 0):.3f}, å…³é”®è¯={result.get('keyword_bonus', 0):.3f}, å…¬å¸={result.get('company_bonus', 0):.3f}, å®ä½“={result.get('entity_type_bonus', 0):.3f}, ç»¼åˆ={result['combined_score']:.3f}")
        
        return sorted_results[:limit]
    
    def _detect_language(self, text: str) -> str:
        """ç®€å•çš„è¯­è¨€æ£€æµ‹"""
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼ˆæ— æ­£åˆ™ï¼Œé€å­—ç¬¦åˆ¤æ–­èŒƒå›´ï¼‰
        chinese_chars = sum(1 for ch in text if '\u4e00' <= ch <= '\u9fff')
        total_chars = len(text.replace(' ', ''))
        
        if total_chars == 0:
            return "en"
        
        chinese_ratio = chinese_chars / total_chars
        return "zh" if chinese_ratio > 0.3 else "en"
    
    def _extract_entities(self, text: str) -> List[Entity]:
        """æ™ºèƒ½å¤šè¯­è¨€å®ä½“æå–"""
        entities = []
        
        # æ£€æµ‹æ–‡æœ¬è¯­è¨€
        lang = self._detect_language(text)
        
        # é€‰æ‹©åˆé€‚çš„spaCyæ¨¡å‹
        selected_nlp = None
        if lang == "zh" and "zh_core_web_sm" in self.nlp_models:
            selected_nlp = self.nlp_models["zh_core_web_sm"]
        elif lang == "en" and "en_core_web_sm" in self.nlp_models:
            selected_nlp = self.nlp_models["en_core_web_sm"]
        else:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            selected_nlp = self.nlp
        
        logger.debug(f"ğŸŒ æ£€æµ‹è¯­è¨€: {lang}, ä½¿ç”¨æ¨¡å‹: {selected_nlp.meta['name'] if selected_nlp else 'None'}")
        
        # ä½¿ç”¨spaCyè¿›è¡Œå®ä½“è¯†åˆ«å’Œè¯æ³•åˆ†æ
        if selected_nlp:
            try:
                doc = selected_nlp(text)
                logger.debug(f"ğŸ“ spaCyå¤„ç†æ–‡æœ¬: '{text}' -> {len(doc.ents)} ä¸ªå®ä½“")
                
                # å­˜å‚¨è¯æ³•åˆ†æç»“æœï¼Œä¾›Neo4jä½¿ç”¨
                self._store_linguistic_analysis(doc, text)
                
                if not doc.ents:
                    # å¦‚æœæ²¡æœ‰å®ä½“ï¼Œè®°å½•è¯¦ç»†çš„è¯å…ƒä¿¡æ¯
                    logger.debug("ğŸ” æœªæ‰¾åˆ°å®ä½“ï¼Œè¯å…ƒåˆ†æ:")
                    for token in doc[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªè¯å…ƒ
                        logger.debug(f"   '{token.text}' -> POS: {token.pos_}, TAG: {token.tag_}, ENT_IOB: {token.ent_iob_}")
                
                for ent in doc.ents:
                    entity = Entity(
                        entity_id=f"entity_{hash(ent.text)}",
                        name=ent.text,
                        entity_type=ent.label_,
                        description=f"ä»æ–‡æœ¬ä¸­è¯†åˆ«çš„{ent.label_}å®ä½“"
                    )
                    entities.append(entity)
                    # å®‰å…¨è·å–ç½®ä¿¡åº¦ä¿¡æ¯
                    confidence = "N/A"
                    try:
                        if hasattr(ent._, 'confidence'):
                            confidence = getattr(ent._, 'confidence', 'N/A')
                    except:
                        confidence = "N/A"
                    
                    logger.debug(f"ğŸ·ï¸ spaCyè¯†åˆ«å®ä½“: '{ent.text}' -> {ent.label_} (ç½®ä¿¡åº¦: {confidence})")
                
            except Exception as e:
                logger.warning(f"âš ï¸ spaCyå®ä½“è¯†åˆ«å¤±è´¥: {e}")
                import traceback
                logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„spaCyæ¨¡å‹è¿›è¡Œå®ä½“è¯†åˆ«")
        
        return entities
    
    def _store_linguistic_analysis(self, doc, text: str):
        """å­˜å‚¨spaCyè¯æ³•åˆ†æç»“æœåˆ°Neo4j"""
        if not self.graph_store:
            return
            
        try:
            # ä¸ºæ¯ä¸ªè¯å…ƒåˆ›å»ºèŠ‚ç‚¹
            for token in doc:
                # è·³è¿‡æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
                if token.is_punct or token.is_space:
                    continue
                    
                token_id = f"token_{hash(token.text + token.pos_)}"
                
                # æ·»åŠ è¯å…ƒèŠ‚ç‚¹åˆ°Neo4j
                self.graph_store.add_entity(
                    entity_id=token_id,
                    name=token.text,
                    entity_type="TOKEN",
                    properties={
                        "pos": token.pos_,        # è¯æ€§ï¼ˆNOUN, VERBç­‰ï¼‰
                        "tag": token.tag_,        # ç»†ç²’åº¦æ ‡ç­¾
                        "lemma": token.lemma_,    # è¯å…ƒåŸå½¢
                        "is_alpha": token.is_alpha,
                        "is_stop": token.is_stop,
                        "source_text": text[:50],  # æ¥æºæ–‡æœ¬ç‰‡æ®µ
                        "language": self._detect_language(text)
                    }
                )
                
                # å¦‚æœæ˜¯åè¯ï¼Œå¯èƒ½æ˜¯æ½œåœ¨çš„æ¦‚å¿µ
                if token.pos_ in ["NOUN", "PROPN"]:
                    concept_id = f"concept_{hash(token.text)}"
                    self.graph_store.add_entity(
                        entity_id=concept_id,
                        name=token.text,
                        entity_type="CONCEPT",
                        properties={
                            "category": token.pos_,
                            "frequency": 1,  # å¯ä»¥åç»­ç´¯è®¡
                            "source_text": text[:50]
                        }
                    )
                    
                    # å»ºç«‹è¯å…ƒåˆ°æ¦‚å¿µçš„å…³ç³»
                    self.graph_store.add_relationship(
                        from_entity_id=token_id,
                        to_entity_id=concept_id,
                        relationship_type="REPRESENTS",
                        properties={"confidence": 1.0}
                    )
            
            # å»ºç«‹è¯å…ƒä¹‹é—´çš„ä¾å­˜å…³ç³»
            for token in doc:
                if token.is_punct or token.is_space or token.head == token:
                    continue
                    
                from_id = f"token_{hash(token.text + token.pos_)}"
                to_id = f"token_{hash(token.head.text + token.head.pos_)}"
                
                # Neo4jä¸å…è®¸å…³ç³»ç±»å‹åŒ…å«å†’å·ï¼Œéœ€è¦æ¸…ç†
                relation_type = token.dep_.upper().replace(":", "_")
                
                self.graph_store.add_relationship(
                    from_entity_id=from_id,
                    to_entity_id=to_id,
                    relationship_type=relation_type,  # æ¸…ç†åçš„ä¾å­˜å…³ç³»ç±»å‹
                    properties={
                        "dependency": token.dep_,  # ä¿ç•™åŸå§‹ä¾å­˜å…³ç³»
                        "source_text": text[:50]
                    }
                )
            
            logger.debug(f"ğŸ”— å·²å°†è¯æ³•åˆ†æç»“æœå­˜å‚¨åˆ°Neo4j: {len([t for t in doc if not t.is_punct and not t.is_space])} ä¸ªè¯å…ƒ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ å­˜å‚¨è¯æ³•åˆ†æå¤±è´¥: {e}")
    
    def _extract_relations(self, text: str, entities: List[Entity]) -> List[Relation]:
        """æå–å…³ç³»"""
        relations = []
        # ä»…ä¿ç•™ç®€å•å…±ç°å…³ç³»ï¼Œä¸åšä»»ä½•æ­£åˆ™/å…³é”®è¯åŒ¹é…
        for i, entity1 in enumerate(entities):
            for entity2 in entities[i+1:]:
                relations.append(Relation(
                    from_entity=entity1.entity_id,
                    to_entity=entity2.entity_id,
                    relation_type="CO_OCCURS",
                    strength=0.5,
                    evidence=text[:100]
                ))
        return relations
    
    def _add_entity_to_graph(self, entity: Entity, memory_item: MemoryItem):
        """æ·»åŠ å®ä½“åˆ°Neo4jå›¾æ•°æ®åº“"""
        try:
            # å‡†å¤‡å®ä½“å±æ€§
            properties = {
                "name": entity.name,
                "description": entity.description,
                "frequency": entity.frequency,
                "memory_id": memory_item.id,
                "user_id": memory_item.user_id,
                "importance": memory_item.importance,
                **entity.properties
            }
            
            # æ·»åŠ åˆ°Neo4j
            success = self.graph_store.add_entity(
                entity_id=entity.entity_id,
                name=entity.name,
                entity_type=entity.entity_type,
                properties=properties
            )
            
            if success:
                # åŒæ—¶æ›´æ–°æœ¬åœ°ç¼“å­˜
                if entity.entity_id in self.entities:
                    self.entities[entity.entity_id].frequency += 1
                    self.entities[entity.entity_id].updated_at = datetime.now()
                else:
                    self.entities[entity.entity_id] = entity
                    
            return success
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å®ä½“åˆ°å›¾æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def _add_relation_to_graph(self, relation: Relation, memory_item: MemoryItem):
        """æ·»åŠ å…³ç³»åˆ°Neo4jå›¾æ•°æ®åº“"""
        try:
            # å‡†å¤‡å…³ç³»å±æ€§
            properties = {
                "strength": relation.strength,
                "memory_id": memory_item.id,
                "user_id": memory_item.user_id,
                "importance": memory_item.importance,
                "evidence": relation.evidence
            }
            
            # æ·»åŠ åˆ°Neo4j
            success = self.graph_store.add_relationship(
                from_entity_id=relation.from_entity,
                to_entity_id=relation.to_entity,
                relationship_type=relation.relation_type,
                properties=properties
            )
            
            if success:
                # åŒæ—¶æ›´æ–°æœ¬åœ°ç¼“å­˜
                self.relations.append(relation)
                
            return success
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å…³ç³»åˆ°å›¾æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def _calculate_graph_relevance_neo4j(self, memory_metadata: Dict[str, Any], query_entities: List[Entity]) -> float:
        """è®¡ç®—Neo4jå›¾ç›¸å…³æ€§åˆ†æ•°"""
        try:
            memory_entities = memory_metadata.get("entities", [])
            if not memory_entities or not query_entities:
                return 0.0
            
            # å®ä½“åŒ¹é…åº¦
            query_entity_ids = {e.entity_id for e in query_entities}
            matching_entities = len(set(memory_entities).intersection(query_entity_ids))
            entity_score = matching_entities / len(query_entity_ids) if query_entity_ids else 0
            
            # å®ä½“æ•°é‡åŠ æƒ
            entity_count = memory_metadata.get("entity_count", 0)
            entity_density = min(entity_count / 10, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
            
            # å…³ç³»æ•°é‡åŠ æƒ
            relation_count = memory_metadata.get("relation_count", 0)
            relation_density = min(relation_count / 5, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
            
            # ç»¼åˆåˆ†æ•°
            relevance_score = (
                entity_score * 0.6 +           # å®ä½“åŒ¹é…æƒé‡60%
                entity_density * 0.2 +         # å®ä½“å¯†åº¦æƒé‡20%
                relation_density * 0.2         # å…³ç³»å¯†åº¦æƒé‡20%
            )
            
            return min(relevance_score, 1.0)
            
        except Exception as e:
            logger.debug(f"è®¡ç®—å›¾ç›¸å…³æ€§å¤±è´¥: {e}")
            return 0.0

    def _add_or_update_entity(self, entity: Entity):
        """æ·»åŠ æˆ–æ›´æ–°å®ä½“"""
        if entity.entity_id in self.entities:
            # æ›´æ–°ç°æœ‰å®ä½“
            existing = self.entities[entity.entity_id]
            existing.frequency += 1
            existing.updated_at = datetime.now()
        else:
            # æ·»åŠ æ–°å®ä½“
            self.entities[entity.entity_id] = entity
    
    def _add_or_update_relation(self, relation: Relation):
        """æ·»åŠ æˆ–æ›´æ–°å…³ç³»"""
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå…³ç³»
        existing_relation = None
        for r in self.relations:
            if (r.from_entity == relation.from_entity and
                r.to_entity == relation.to_entity and
                r.relation_type == relation.relation_type):
                existing_relation = r
                break
        
        if existing_relation:
            # æ›´æ–°ç°æœ‰å…³ç³»
            existing_relation.frequency += 1
            existing_relation.strength = min(1.0, existing_relation.strength + 0.1)
        else:
            # æ·»åŠ æ–°å…³ç³»
            self.relations.append(relation)
    
    # æ—§çš„å›¾ç›¸å…³æ€§è®¡ç®—æ–¹æ³•å·²è¢« _calculate_graph_relevance_neo4j æ›¿ä»£
    
    def _find_memory_by_id(self, memory_id: str) -> Optional[MemoryItem]:
        """æ ¹æ®IDæŸ¥æ‰¾è®°å¿†"""
        logger.debug(f"ğŸ” æŸ¥æ‰¾è®°å¿†ID: {memory_id}, å½“å‰è®°å¿†æ•°: {len(self.semantic_memories)}")
        for memory in self.semantic_memories:
            if memory.id == memory_id:
                logger.debug(f"âœ… æ‰¾åˆ°è®°å¿†: {memory.content[:50]}...")
                return memory
        logger.debug(f"âŒ æœªæ‰¾åˆ°è®°å¿†ID: {memory_id}")
        return None
    
    def update(
        self,
        memory_id: str,
        content: str = None,
        importance: float = None,
        metadata: Dict[str, Any] = None
    ) -> bool:
        """æ›´æ–°è¯­ä¹‰è®°å¿†"""
        memory = self._find_memory_by_id(memory_id)
        if not memory:
            return False
        
        try:
            if content is not None:
                # é‡æ–°ç”ŸæˆåµŒå…¥å’Œæå–å®ä½“
                embedding = self.embedding_model.encode(content)
                self.memory_embeddings[memory_id] = embedding
                
                # æ¸…ç†æ—§çš„å®ä½“å…³ç³»
                old_entities = memory.metadata.get("entities", [])
                self._cleanup_entities_and_relations(old_entities)
                
                # æå–æ–°çš„å®ä½“å’Œå…³ç³»
                memory.content = content
                entities = self._extract_entities(content)
                relations = self._extract_relations(content, entities)
                
                # æ›´æ–°çŸ¥è¯†å›¾è°±
                for entity in entities:
                    self._add_or_update_entity(entity)
                for relation in relations:
                    self._add_or_update_relation(relation)
                
                # æ›´æ–°å…ƒæ•°æ®
                memory.metadata["entities"] = [e.entity_id for e in entities]
                memory.metadata["relations"] = [
                    f"{r.from_entity}-{r.relation_type}-{r.to_entity}" for r in relations
                ]
                
            if importance is not None:
                memory.importance = importance
            
            if metadata is not None:
                memory.metadata.update(metadata)
                
                return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°è®°å¿†å¤±è´¥: {e}")
        return False
    
    def remove(self, memory_id: str) -> bool:
        """åˆ é™¤è¯­ä¹‰è®°å¿†"""
        memory = self._find_memory_by_id(memory_id)
        if not memory:
            return False
        
        try:
            # åˆ é™¤å‘é‡
            self.vector_store.delete_memories([memory_id])
            
            # æ¸…ç†å®ä½“å’Œå…³ç³»
            entities = memory.metadata.get("entities", [])
            self._cleanup_entities_and_relations(entities)
            
            # åˆ é™¤è®°å¿†
            self.semantic_memories.remove(memory)
            if memory_id in self.memory_embeddings:
                del self.memory_embeddings[memory_id]
                
                return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤è®°å¿†å¤±è´¥: {e}")
        return False
    
    def _cleanup_entities_and_relations(self, entity_ids: List[str]):
        """æ¸…ç†å®ä½“å’Œå…³ç³»"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´æ™ºèƒ½çš„æ¸…ç†é€»è¾‘
        # ä¾‹å¦‚ï¼Œå¦‚æœå®ä½“ä¸å†è¢«ä»»ä½•è®°å¿†å¼•ç”¨ï¼Œåˆ™åˆ é™¤å®ƒ
        pass
    
    def has_memory(self, memory_id: str) -> bool:
        """æ£€æŸ¥è®°å¿†æ˜¯å¦å­˜åœ¨"""
        return self._find_memory_by_id(memory_id) is not None
    
    def forget(self, strategy: str = "importance_based", threshold: float = 0.1, max_age_days: int = 30) -> int:
        """è¯­ä¹‰è®°å¿†é—å¿˜æœºåˆ¶ï¼ˆç¡¬åˆ é™¤ï¼‰"""
        forgotten_count = 0
        current_time = datetime.now()
        
        to_remove = []  # æ”¶é›†è¦åˆ é™¤çš„è®°å¿†ID
        
        for memory in self.semantic_memories:
            should_forget = False
            
            if strategy == "importance_based":
                # åŸºäºé‡è¦æ€§é—å¿˜
                if memory.importance < threshold:
                    should_forget = True
            elif strategy == "time_based":
                # åŸºäºæ—¶é—´é—å¿˜
                cutoff_time = current_time - timedelta(days=max_age_days)
                if memory.timestamp < cutoff_time:
                    should_forget = True
            elif strategy == "capacity_based":
                # åŸºäºå®¹é‡é—å¿˜ï¼ˆä¿ç•™æœ€é‡è¦çš„ï¼‰
                if len(self.semantic_memories) > self.config.max_capacity:
                    sorted_memories = sorted(self.semantic_memories, key=lambda m: m.importance)
                    excess_count = len(self.semantic_memories) - self.config.max_capacity
                    if memory in sorted_memories[:excess_count]:
                        should_forget = True
            
            if should_forget:
                to_remove.append(memory.id)
        
        # æ‰§è¡Œç¡¬åˆ é™¤
        for memory_id in to_remove:
            if self.remove(memory_id):
                forgotten_count += 1
                logger.info(f"è¯­ä¹‰è®°å¿†ç¡¬åˆ é™¤: {memory_id[:8]}... (ç­–ç•¥: {strategy})")
        
        return forgotten_count

    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰è¯­ä¹‰è®°å¿† - åŒ…æ‹¬ä¸“ä¸šæ•°æ®åº“"""
        try:
            # æ¸…ç©ºQdrantå‘é‡æ•°æ®åº“
            if self.vector_store:
                success = self.vector_store.clear_collection()
                if success:
                    logger.info("âœ… Qdrantå‘é‡æ•°æ®åº“å·²æ¸…ç©º")
                else:
                    logger.warning("âš ï¸ Qdrantæ¸…ç©ºå¤±è´¥")
            
            # æ¸…ç©ºNeo4jå›¾æ•°æ®åº“
            if self.graph_store:
                success = self.graph_store.clear_all()
                if success:
                    logger.info("âœ… Neo4jå›¾æ•°æ®åº“å·²æ¸…ç©º")
                else:
                    logger.warning("âš ï¸ Neo4jæ¸…ç©ºå¤±è´¥")
            
            # æ¸…ç©ºæœ¬åœ°ç¼“å­˜
            self.semantic_memories.clear()
            self.memory_embeddings.clear()
            self.entities.clear()
            self.relations.clear()
            
            logger.info("ğŸ§¹ è¯­ä¹‰è®°å¿†ç³»ç»Ÿå·²å®Œå…¨æ¸…ç©º")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºè¯­ä¹‰è®°å¿†å¤±è´¥: {e}")
            # å³ä½¿æ•°æ®åº“æ¸…ç©ºå¤±è´¥ï¼Œä¹Ÿè¦æ¸…ç©ºæœ¬åœ°ç¼“å­˜
        self.semantic_memories.clear()
        self.memory_embeddings.clear()
        self.entities.clear()
        self.relations.clear()

    def get_all(self) -> List[MemoryItem]:
        """è·å–æ‰€æœ‰è¯­ä¹‰è®°å¿†"""
        return self.semantic_memories.copy()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è¯­ä¹‰è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        graph_stats = {}
        try:
            if self.graph_store:
                graph_stats = self.graph_store.get_stats() or {}
        except Exception:
            graph_stats = {}

        # ç¡¬åˆ é™¤æ¨¡å¼ï¼šæ‰€æœ‰è®°å¿†éƒ½æ˜¯æ´»è·ƒçš„
        active_memories = self.semantic_memories

        return {
            "count": len(active_memories),  # æ´»è·ƒè®°å¿†æ•°é‡
            "forgotten_count": 0,  # ç¡¬åˆ é™¤æ¨¡å¼ä¸‹å·²é—å¿˜çš„è®°å¿†ä¼šè¢«ç›´æ¥åˆ é™¤
            "total_count": len(self.semantic_memories),  # æ€»è®°å¿†æ•°é‡
            "entities_count": len(self.entities),
            "relations_count": len(self.relations),
            "graph_nodes": graph_stats.get("total_nodes", 0),
            "graph_edges": graph_stats.get("total_relationships", 0),
            "avg_importance": sum(m.importance for m in active_memories) / len(active_memories) if active_memories else 0.0,
            "memory_type": "enhanced_semantic"
        }
    
    def get_entity(self, entity_id: str) -> Optional[Entity]:
        """è·å–å®ä½“"""
        return self.entities.get(entity_id)
    
    def search_entities(self, query: str, limit: int = 10) -> List[Entity]:
        """æœç´¢å®ä½“"""
        query_lower = query.lower()
        scored_entities = []
        
        for entity in self.entities.values():
            score = 0.0
            
            # åç§°åŒ¹é…
            if query_lower in entity.name.lower():
                score += 2.0
            
            # ç±»å‹åŒ¹é…
            if query_lower in entity.entity_type.lower():
                score += 1.0
            
            # æè¿°åŒ¹é…
            if query_lower in entity.description.lower():
                score += 0.5
            
            # é¢‘ç‡æƒé‡
            score *= math.log(1 + entity.frequency)
            
            if score > 0:
                scored_entities.append((score, entity))
        
        scored_entities.sort(key=lambda x: x[0], reverse=True)
        return [entity for _, entity in scored_entities[:limit]]
    
    def get_related_entities(
        self,
        entity_id: str,
        relation_types: List[str] = None,
        max_hops: int = 2
    ) -> List[Dict[str, Any]]:
        """è·å–ç›¸å…³å®ä½“ - ä½¿ç”¨Neo4jå›¾æ•°æ®åº“"""
        
        related = []
        
        try:
            # ä½¿ç”¨Neo4jå›¾æ•°æ®åº“æŸ¥æ‰¾ç›¸å…³å®ä½“
            if not self.graph_store:
                logger.warning("âš ï¸ Neo4jå›¾æ•°æ®åº“ä¸å¯ç”¨")
                return []
            
            # ä½¿ç”¨Neo4jæŸ¥æ‰¾ç›¸å…³å®ä½“
            related_entities = self.graph_store.find_related_entities(
                entity_id=entity_id,
                relationship_types=relation_types,
                max_depth=max_hops,
                limit=50
            )
            
            # è½¬æ¢æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            for entity_data in related_entities:
                # å°è¯•ä»æœ¬åœ°ç¼“å­˜è·å–å®ä½“å¯¹è±¡
                entity_obj = self.entities.get(entity_data.get("id"))
                if not entity_obj:
                    # å¦‚æœæœ¬åœ°ç¼“å­˜æ²¡æœ‰ï¼Œåˆ›å»ºä¸´æ—¶å®ä½“å¯¹è±¡
                    entity_obj = Entity(
                        entity_id=entity_data.get("id", entity_id),
                        name=entity_data.get("name", ""),
                        entity_type=entity_data.get("type", "MISC")
                    )
                
                    related.append({
                    "entity": entity_obj,
                    "relation_type": entity_data.get("relationship_path", ["RELATED"])[-1] if entity_data.get("relationship_path") else "RELATED",
                    "strength": 1.0 / max(entity_data.get("distance", 1), 1),  # è·ç¦»è¶Šè¿‘å¼ºåº¦è¶Šé«˜
                    "distance": entity_data.get("distance", max_hops)
                })
            
            # æŒ‰è·ç¦»å’Œå¼ºåº¦æ’åº
            related.sort(key=lambda x: (x["distance"], -x["strength"]))
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç›¸å…³å®ä½“å¤±è´¥: {e}")
        
        return related
    
    def export_knowledge_graph(self) -> Dict[str, Any]:
        """å¯¼å‡ºçŸ¥è¯†å›¾è°± - ä»Neo4jè·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ä»Neo4jè·å–ç»Ÿè®¡ä¿¡æ¯
            stats = {}
            if self.graph_store:
                stats = self.graph_store.get_stats()
            
            return {
                "entities": {eid: entity.to_dict() for eid, entity in self.entities.items()},
                "relations": [relation.to_dict() for relation in self.relations],
                "graph_stats": {
                    "total_nodes": stats.get("total_nodes", 0),
                    "entity_nodes": stats.get("entity_nodes", 0),
                    "memory_nodes": stats.get("memory_nodes", 0),
                    "total_relationships": stats.get("total_relationships", 0),
                    "cached_entities": len(self.entities),
                    "cached_relations": len(self.relations)
                }
            }
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return {
                "entities": {},
                "relations": [],
                "graph_stats": {"error": str(e)}
            }
